{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9cd323e",
   "metadata": {},
   "source": [
    "# Training a Machine Learning Model\n",
    "Training machine learning models to predict FPL player points using engineered features.\n",
    "\n",
    "## Models to train\n",
    "- Random Forest, ensemble of decision trees \n",
    "- Linear Regression, baseline for comparison\n",
    "\n",
    "## Features\n",
    "- rolling_avg_points, player form last 5 games \n",
    "- opponent_difficulty, fixture difficulty (1 -10 scale)\n",
    "- minutes, playing time a player gets (0-90 minutes)\n",
    "\n",
    "## Steps\n",
    "- Load features from 'fpl_features.csv',split the data into two parts 80% training and 20% testing, then test both models on the training data, assess on the test data & compare the performance.\n",
    "\n",
    "## Performance Metrics \n",
    "- MAE (Mean Absolute Error), How far the predictions were off by \n",
    "- RMSE (Root Mean Squared Error), Prediction errors\n",
    "- R^2 Score, How well the model predicts (0-1, higher score is better)\n",
    "\n",
    "## Output\n",
    "Random Forest normally achieves - \n",
    "MAE, 2.0-2.5 points\n",
    "R^2 Score, 0.45-.055"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f0014d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries for ML training\n",
    "# pandas for data manipulation, numpy for numerical operations\n",
    "# sklearn for machine learning models and evaluation\n",
    "# matplotlib for visualizations\n",
    "# pickle to save the machine learning models\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ead538c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10329 records\n",
      "756 players\n",
      "1 to 14 GW\n",
      "   name  gameweek  rolling_avg_points  opponent_difficulty  minutes  \\\n",
      "0  Raya         1                 0.0                  6.9       90   \n",
      "1  Raya         2                 0.0                  6.5       90   \n",
      "2  Raya         3                 0.0                  4.9       90   \n",
      "3  Raya         4                 0.0                  7.9       90   \n",
      "4  Raya         5                 0.0                  1.2       90   \n",
      "\n",
      "   total_points  \n",
      "0            10  \n",
      "1             6  \n",
      "2             2  \n",
      "3             6  \n",
      "4             2  \n"
     ]
    }
   ],
   "source": [
    "# Loading the features that are in the featureengineering.ipynb\n",
    "# rolling averages, opponent difficulty and minutes\n",
    "# these will be used to train the model\n",
    "\n",
    "# reading the csv file that has all the features\n",
    "df = pd.read_csv('fpl_features.csv')\n",
    "\n",
    "# Showing how many records, players & gameweeks were loaded\n",
    "print(f\"{len(df)} records\")\n",
    "print(f\"{df['player_id'].nunique()} players\")\n",
    "print(f\"{df['gameweek'].min()} to {df['gameweek'].max()} GW\")\n",
    "\n",
    "# showing the data loaded properly \n",
    "print(df[['name', 'gameweek', 'rolling_avg_points', 'opponent_difficulty', \n",
    "          'minutes', 'total_points']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "000904a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features (x) ['rolling_avg_points', 'opponent_difficulty', 'minutes']\n",
      "target (y) total points\n",
      "\n",
      "training set: 8263 records (80%)\n",
      "test set: 2066 records (20%)\n"
     ]
    }
   ],
   "source": [
    "# preparing data for machine learning\n",
    "# splitting it into features x and y\n",
    "# removing players who didnt play\n",
    "\n",
    "# features x and y\n",
    "# x - input features (what the moodel learns from)\n",
    "# y - target variable (what the model iis trying to predict)\n",
    "# putting the features into x for the model to learn from\n",
    "features = ['rolling_avg_points', 'opponent_difficulty', 'minutes']\n",
    "x = df[features]\n",
    "y = df['total_points']\n",
    "\n",
    "# showing the features we are using for the model \n",
    "# and the total points\n",
    "print(f\"features (x) {features}\")\n",
    "print(f\"target (y) total points\")\n",
    "\n",
    "# splitting data into two parts training 80% and testing 20%\n",
    "# training set, model learns patterns from this data\n",
    "# testing set, model evaluates on this to test generalization\n",
    "# making the random_state - 42, makes the split reproducible\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\ntraining set: {len(x_train)} records (80%)\")\n",
    "print(f\"test set: {len(x_test)} records (20%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "10ed35e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE (mean absolute error):  0.83 points\n",
      "On average, predictions are off by 0.83 points\n",
      "RMSE (root mean squared error): 2.01 points\n",
      "R² Score: 0.315\n",
      "Model shows how well the model predicts points 31.5%\n",
      "Moderate performance, Model explains 31.5% of variance\n",
      "Sample Predictions (Random Forest):\n",
      "   Actual  Predicted  Error\n",
      "0       0        0.0    0.0\n",
      "1       0        0.0    0.0\n",
      "2       1        1.3    0.3\n",
      "3       1        7.1    6.1\n",
      "4       1        1.5    0.5\n",
      "5       2        1.7    0.3\n",
      "6       0        0.0    0.0\n",
      "7       0        0.0    0.0\n",
      "8       0        0.0    0.0\n",
      "9       0        0.0    0.0\n"
     ]
    }
   ],
   "source": [
    "# implementing random forest an ensemble learning method\n",
    "# makes multiple decision trees & averages their predictions\n",
    "# it is good at capturing non-linear relationships\n",
    "# using 100 n_estimators, which is 100 decision trees\n",
    "# random_state=42 for reproducability\n",
    "# n_jobs=1 uses all cpu cores for faster training\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "# fit() trains the model on the training data\n",
    "rf_model.fit(x_train, y_train)\n",
    "\n",
    "# predictions on test data\n",
    "# showing how well the model works on unseen data\n",
    "rf_predictions = rf_model.predict(x_test)\n",
    "\n",
    "# showing model performance using standard metrics\n",
    "# MAE (mean absolute error) average prediction error in points\n",
    "# the lower the better, shows the average points it is off by\n",
    "rf_mae = mean_absolute_error(y_test, rf_predictions)\n",
    "print(f\"MAE (mean absolute error):  {rf_mae:.2f} points\")\n",
    "print(f\"On average, predictions are off by {rf_mae:.2f} points\")\n",
    "\n",
    "# RMSE (root mean squared error) penalizes large errors more than mae\n",
    "# the lower the better\n",
    "rf_rmse = np.sqrt(mean_squared_error(y_test, rf_predictions))\n",
    "print(f\"RMSE (root mean squared error): {rf_rmse:.2f} points\")\n",
    "\n",
    "# R^2 Score, how well the model predicts\n",
    "# 1.0 means perfect predictions, 0.0 means the model is the same as just getting the average\n",
    "# the higher the bettter\n",
    "rf_r2 = r2_score(y_test, rf_predictions)\n",
    "print(f\"R² Score: {rf_r2:.3f}\")\n",
    "print(f\"Model shows how well the model predicts points {rf_r2*100:.1f}%\")\n",
    "\n",
    "# Finding the R² score\n",
    "if rf_r2 > 0.5:\n",
    "    print(f\"Strong performance, Model explains {rf_r2*100:.1f}% of variance\")\n",
    "elif rf_r2 > 0.3:\n",
    "    print(f\"Moderate performance, Model explains {rf_r2*100:.1f}% of variance\")\n",
    "elif rf_r2 > 0:\n",
    "    print(f\"Weak performance, Model explains {rf_r2*100:.1f}% of variance\")\n",
    "else:\n",
    "    print(f\"Poor performance, Model didn't learn (negative R²)\")\n",
    "\n",
    "\n",
    "# showing the predictions to see how close they are \n",
    "print(\"Sample Predictions (Random Forest):\")\n",
    "comparison = pd.DataFrame({\n",
    "    'Actual': y_test[:10].values,\n",
    "    'Predicted': rf_predictions[:10].round(1),\n",
    "    'Error': abs(y_test[:10].values - rf_predictions[:10]).round(1)\n",
    "})\n",
    "print(comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7816dc60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE (mean absolute error):  0.86 points\n",
      "RMSE (root mean squared error): 1.78 points\n",
      "R² Score: 0.462\n",
      "Model shows how well the model predicts points 31.5%\n",
      "\n",
      "Example Predictions (Linear Regression):\n",
      "------------------------------------------------------------\n",
      "   Actual  Predicted\n",
      "0       0        0.1\n",
      "1       0        0.0\n",
      "2       1        1.4\n",
      "3       1        4.2\n",
      "4       1        0.7\n",
      "5       2        3.6\n",
      "6       0        0.0\n",
      "7       0        0.1\n",
      "8       0        0.2\n",
      "9       0       -0.1\n"
     ]
    }
   ],
   "source": [
    "# Making a baseline (Linear Regression) to compare ml models to\n",
    "# This model assumes a straight line relationship between \n",
    "# inputs and outputs\n",
    "\n",
    "# training linear regression model\n",
    "# no features needed\n",
    "lr_model = LinearRegression()\n",
    "# fit() finds the best linear equation to fit the data\n",
    "lr_model.fit(x_train, y_train)\n",
    "\n",
    "# showing how well the model works\n",
    "lr_predictions = lr_model.predict(x_test)\n",
    "\n",
    "# showing model performance using standard regression metrics\n",
    "# MAE (mean absolute error) average prediction error in points\n",
    "# the lower the better, shows the average points it is off by\n",
    "lr_mae = mean_absolute_error(y_test, lr_predictions)\n",
    "print(f\"MAE (mean absolute error):  {lr_mae:.2f} points\")\n",
    "\n",
    "# RMSE (root mean squared error) penalizes large errors more than mae\n",
    "# the lower the better\n",
    "lr_rmse = np.sqrt(mean_squared_error(y_test, lr_predictions))\n",
    "print(f\"RMSE (root mean squared error): {lr_rmse:.2f} points\")\n",
    "\n",
    "# R^2 Score, how well the model predicts\n",
    "# 1.0 means perfect predictions, 0.0 means the model is the same as just getting the average\n",
    "# the higher the bettter\n",
    "lr_r2 = r2_score(y_test, lr_predictions)\n",
    "print(f\"R² Score: {lr_r2:.3f}\")\n",
    "print(f\"Model shows how well the model predicts points {rf_r2*100:.1f}%\")\n",
    "\n",
    "# showing the predictions to see how close they are \n",
    "print(\"\\nExample Predictions (Linear Regression):\")\n",
    "print(\"-\"*60)\n",
    "comparison = pd.DataFrame({\n",
    "    'Actual': y_test[:10].values,\n",
    "    'Predicted': lr_predictions[:10].round(1)\n",
    "})\n",
    "print(comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "36f96628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparing Random Forest & Linear Regression\n",
      "            Model      MAE     RMSE  R² Score\n",
      "    Random Forest 0.825537 2.006421  0.315409\n",
      "Linear Regression 0.859606 1.779066  0.461767\n",
      "\n",
      "Best Model (Lowest MAE): Random Forest\n",
      "\n",
      "Feature Importance (Random Forest):\n",
      "            Feature  Importance\n",
      "            minutes    0.703916\n",
      " rolling_avg_points    0.164969\n",
      "opponent_difficulty    0.131115\n",
      "\n",
      "Explanation:\n",
      "minutes: 70.4% importance\n",
      "rolling_avg_points: 16.5% importance\n",
      "opponent_difficulty: 13.1% importance\n"
     ]
    }
   ],
   "source": [
    "# Comparing both models to see the best one\n",
    "# The model that has the lowest MAE score is the best\n",
    "# in fpl the average error matters more than then perfect predicition\n",
    "\n",
    "# creating a comparison dataframe/table with all the metrics\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': ['Random Forest', 'Linear Regression'],\n",
    "    'MAE': [rf_mae, lr_mae],\n",
    "    'RMSE': [rf_rmse, lr_rmse],\n",
    "    'R² Score': [rf_r2, lr_r2]\n",
    "})\n",
    "\n",
    "# Printing out the comparison dataframe/table\n",
    "print(\"\\nComparing Random Forest & Linear Regression\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Seeing what the best model is, the lowest MAE score\n",
    "# The lower the MAE , the smaller average prediction error\n",
    "bmn = comparison_df.loc[comparison_df['MAE'].idxmin(), 'Model']\n",
    "print(f\"\\nBest Model (Lowest MAE): {bmn}\")\n",
    "\n",
    "# This shows what feature matters the most\n",
    "print(\"\\nFeature Importance (Random Forest):\")\n",
    "# Geting the performance score of each feature\n",
    "# The higher the score th more important they are\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Importance': rf_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(feature_importance.to_string(index=False))\n",
    "\n",
    "# explaining what the importance of the features\n",
    "print(\"\\nExplanation:\")\n",
    "# for the features in the table show what % of performance each has\n",
    "for idx, row in feature_importance.iterrows():\n",
    "    print(f\"{row['Feature']}: {row['Importance']*100:.1f}% importance\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b512112d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using gameweek 14 data for predictions\n",
      "\n",
      "Top 10 players, Actual v Predicted\n",
      "       Player  Form  Opponent_Diff  Minutes  Actual_Points  Predicted_Points\n",
      "          Eze   7.6            6.3        0              0               0.0\n",
      "       Thiago   7.4            1.0        0              0               0.0\n",
      "     Casemiro   7.2            7.8        0              0               0.0\n",
      "      O.Dango   6.8            1.0        0              0               0.0\n",
      "     Martinez   6.6            7.4        0              0               0.0\n",
      "Dewsbury-Hall   6.6            5.4       90              3               3.1\n",
      "         Rice   6.4            6.3        0              0               0.0\n",
      "      Lacroix   6.4            8.0        0              0               0.0\n",
      "     Bruno G.   6.4            4.6       45              7               5.0\n",
      "       Minteh   6.2            6.6        0              0               0.0\n",
      "\n",
      "Average prediction error. 0.21 points\n"
     ]
    }
   ],
   "source": [
    "# Testing random forest on actual players\n",
    "# Getting the latest gameweek data for realistic testing\n",
    "latest_gw = df['gameweek'].max()\n",
    "print(f\"\\nUsing gameweek {latest_gw} data for predictions\")\n",
    "\n",
    "# getting the top 10 players by their form\n",
    "players = df[df['gameweek'] == latest_gw].nlargest(10, 'rolling_avg_points')\n",
    "\n",
    "# adding the features to the players \n",
    "x_players = players[features]\n",
    "# generating precitions using random forest\n",
    "predictions_rf = rf_model.predict(x_players)\n",
    "\n",
    "# making a predictions dataframe/table to show to actual points v the predicted points  \n",
    "predictions = pd.DataFrame({\n",
    "    'Player': players['name'].values,\n",
    "    'Form': players['rolling_avg_points'].values,\n",
    "    'Opponent_Diff': players['opponent_difficulty'].values,\n",
    "    'Minutes': players['minutes'].values,\n",
    "    'Actual_Points': players['total_points'].values,\n",
    "    'Predicted_Points': predictions_rf.round(1)\n",
    "})\n",
    "\n",
    "# printing out the predictions table of actual points v predicted point\n",
    "print(\"\\nTop 10 players, Actual v Predicted\")\n",
    "print(predictions.to_string(index=False))\n",
    "\n",
    "# calculating thhe average error of these predictions\n",
    "# showing how accurate random forest is on the players\n",
    "predictions['Error'] = abs(predictions['Actual_Points'] - predictions['Predicted_Points'])\n",
    "print(f\"\\nAverage prediction error. {predictions['Error'].mean():.2f} points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "11903d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest saved to fpl_predictor_model.pkl\n",
      "Summary of Random Forest saved to model_summary.txt\n"
     ]
    }
   ],
   "source": [
    "# Saving Random Forest using pickle\n",
    "# saving to pickle so can the load the file later to make predictions without retraining\n",
    "model_filename = 'fpl_predictor_model.pkl'\n",
    "# wb to write the file in binary mode\n",
    "with open(model_filename, 'wb') as file:\n",
    "    pickle.dump(rf_model, file)\n",
    "\n",
    "print(f\"Random Forest saved to {model_filename}\")\n",
    "\n",
    "# creating a summary of the ml training\n",
    "summary = f\"\"\"\n",
    "Random Forest\n",
    "Training Date - {pd.Timestamp.now()}\n",
    "\n",
    "Features Used -\n",
    "rolling_avg_points - Player form from the last 5 games\n",
    "opponent_difficulty - Fixture difficulty from a scale of 1-10\n",
    "minutes - How many minutes a player gets from 0 -90 minutes\n",
    "\n",
    "Performance Metrics -\n",
    "MAE - {rf_mae:.2f} points\n",
    "RMSE - {rf_rmse:.2f} points\n",
    "R² Score - {rf_r2:.3f}\n",
    "\n",
    "Training Data -\n",
    "- Records: {len(x_train)}\n",
    "- Players: {df['player_id'].nunique()}\n",
    "- Gameweeks: {df['gameweek'].min()} to {df['gameweek'].max()}\n",
    "\"\"\"\n",
    "\n",
    "# saving the summary to a txt file\n",
    "with open('model_summary.txt', 'w') as f:\n",
    "    f.write(summary)\n",
    "\n",
    "print(\"Summary of Random Forest saved to model_summary.txt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
